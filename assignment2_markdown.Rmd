---
title: "Assignment 2"
author: "B166247"
date: "3 3 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Problem 1
## (a)

```{r warning=FALSE}
#loading the required library
library(data.table)

#loading the files
lipids.dt <- data.table(read.delim("lipids.txt"))
classes.dt <- data.table(read.delim("lipid-classes.txt"))

#lipid classes can be found in the first column of lipids.dt
#Finding all lipids PC and assignment to new column lipid.class (42)
lipids.dt[grep("^[Pp]C", lipid.species), lipid.class := classes.dt[CE == "PC", Cholesterol.esters]]
#Finding all Cer (11)
lipids.dt[grep("^[Cc]er|CER", lipid.species), lipid.class := classes.dt[CE == "Cer", Cholesterol.esters]]
#Finding all DAG (16)
lipids.dt[grep("^[Dd]ag|DAG", lipid.species), lipid.class := classes.dt[CE == "DAG", Cholesterol.esters]]
#Finding all LPC (13)
lipids.dt[grep("^[Ll]pc|LPC", lipid.species), lipid.class := classes.dt[CE == "LPC", Cholesterol.esters]]
#Finding all LPE (3)
lipids.dt[grep("^[Ll]pe|LPE", lipid.species), lipid.class := classes.dt[CE == "LPE", Cholesterol.esters]]
#Finding all PE (25)
lipids.dt[grep("^[Pp]e|^PE", lipid.species), lipid.class := classes.dt[CE == "PE", Cholesterol.esters]]
#Finding all PS (8)
lipids.dt[grep("^[Pp]s|^PS", lipid.species), lipid.class := classes.dt[CE == "PS", Cholesterol.esters]]
#Finding all SM (0)
lipids.dt[grep("[Ss]m|SM", lipid.species)]
#Finding all TAG (147)
lipids.dt[grep("[Tt]ag|TAG", lipid.species), lipid.class := classes.dt[CE == "TAG", Cholesterol.esters]]

#it can be seen that 9 rows are still unoccupied (correspond to CE)
lipids.dt[is.na(lipid.class)]

```
After adding all classes provided in the classes.txt file, it can be seen that one class is missing and 9 rows with the abbreviation CE could not be annotated. Following the shorthand notation of lipids published by Liebisch et al. in 2013 CE corresponds to Cholesteryl ester (Liebisch et al., 2013). I added this class manually.
```{r}
#manual addition of Cholesteryl ester
lipids.dt[grep("\\b[C]e\\b|\\bCE\\b", lipid.species), lipid.class := "Cholesteryl ester"]
#test that all lipids have a class
stopifnot(!is.na(lipids.dt$lipid.class))

#count the number of entries corresponding to each class (one class missing only 8!)
lipids.dt[, .(count = .N), lipid.class]

```
## (b)
Since there are 9 lipid species in the table (including CE and exclusing SM), they are calculated as following:
```{r}
#calculate the Wald test (t-distributed)
lipids.dt[, "Wald.test(t.dist)":= round(((oddsratio-1)/se), 3)]

#calculate the degress of freedom
df <- 288-9-1

#calculate the p-value based on the t-distribution
lipids.dt[, "p.value.t" := signif(2*pt(abs(`Wald.test(t.dist)`), df, lower.tail = FALSE), 4)]

#calculate the p-value based on the normal distribution
results.dt <- lipids.dt[, "p.value.norm" := signif(2*pnorm(`Wald.test(t.dist)`, lower.tail = FALSE), 4)]

#checking the table
head(results.dt)
```
Present some evidence to justify if the normal approximation is acceptable in this instance. (ANSWER)

## (c)
```{r warning=FALSE}
holm.bonferroni <- function(results.dt, alpha){
  #copy and order the input dataframe
  order.dt <- results.dt[order(p.value.t),]
  
  #looping through all sorted entries
  for (row.idx in seq(1,nrow(order.dt))){
    #calculate the threshhold value for the row
    pcalc <- alpha/(276+1-row.idx)
    #cat(pcalc)
    #testing if the threshold is passed
    if (pcalc >= order.dt[row.idx, p.value.t]) {
      #threshold is not passed anymore and the loop breaks
      lastidx <- row.idx
      #cat(lastidx, "\n")
      #cat("This is the pcalc ", pcalc)
    } else {
      #nothting happens when the pcalc gets too large
    }
  }
  
  #returning the subset of the ordered table
  return(order.dt[1:lastidx])
}

```
## (d)
```{r warning=FALSE}
benjamini.hochberg <- function(results.dt, q){
  q <- 0.01
  #copy and order the input dataframe
  order.dt <- results.dt[order(p.value.t),]
  
  #looping through all sorted entries
  for (row.idx in seq(1,nrow(order.dt))){
    pcalc <- (row.idx/276)*q
    
    #testing if the threshold is valid
    if (pcalc > order.dt[row.idx, p.value.t]){
      #assigning the indeces that pass the threshold
      lastidx <- row.idx
      #cat(lastidx, "\n")
      #cat("This is the pcalc ", pcalc)
    } else {
      #pass as the threshold was passed
    }
    
  }
  
  #return the subset in ascending p value order
  return(order.dt[1:lastidx])
  
}

```
## (e)
```{r warning=FALSE}

#finding the subsets that are significant
sigholm <- holm.bonferroni(results.dt, 0.05)
sigben <- benjamini.hochberg(results.dt, 0.01)

#plot the volcano plot
plot(log(results.dt$oddsratio), -log10(results.dt$p.value.t), main = "Volcano Plot Lipid Species",
     xlab = "log Odds Ratio",
     ylab = "-log10(p-value)",
     col = ifelse(results.dt$p.value.t %in% c(sigholm$p.value.t, sigben$p.value.t),"blue", "black" ),
     pch = ifelse(results.dt$p.value.t %in% sigholm$p.value.t, 4, 18),
     cex=ifelse((results.dt$p.value.t %in% sigholm$p.value.t | results.dt$p.value.t %in% sigben$p.value.t), 1.3, 0.3))

#add legend
legend("topleft", c("Holm-Bon. < 5%", "Holm-Bon. ≥ 5%", "Ben.-Hoch. < 1%", "Ben.-Hoch. ≥ 1%"), 
       col = c("blue","black", "blue", "black"),
       lwd = c(3,1,5,1),
       lty = c(NA,NA,NA,NA),
       pch = c(4,18,18,18))

```

## (f)
```{r warning=FALSE}
#Lipid Species that are significant after family wise error rate of 0.05
holmlipid.dt <- holm.bonferroni(lipids.dt, 0.05)
holmlipid.dt
```

```{r warning=FALSE}
#Lipid Species significant after false recovery rate of 0.05
benjaminlipid.dt <- benjamini.hochberg(lipids.dt, 0.05)
benjaminlipid.dt

```

```{r warning=FALSE}
#finding the lipid species that are significant with either method
lipid.inter <- which(results.dt$lipid.species %in% intersect(holmlipid.dt$lipid.species, benjaminlipid.dt$lipid.species))
results.dt[lipid.inter]

```

# Problem 2
## (a)
```{r warning=FALSE}
#loading the required libraries
library(caret)
library(pROC)
library(glmnet)

#load the data set
wdbc2.dt <- fread("wdbc2.csv", stringsAsFactors = TRUE)

#setting the seed
set.seed(1)

#create the data particioning
train.idx <- createDataPartition(wdbc2.dt$diagnosis, p=0.7)$Resample1

#function from lab 4 to convert data tables into a matrix as expected by glmnet
prepare.glmnet <- function(data, formula=~ .) {
  ## create the design matrix to deal correctly with factor variables,
  ## without losing rows containing NAs
  old.opts <- options(na.action='na.pass')
  x <- model.matrix(formula, data)
  options(old.opts)
  ## remove the intercept column, as glmnet will add one by default
  x <- x[, -match("(Intercept)", colnames(x))]
  return(x)
}

#separate desig matrix from outcomes
ywdbc2.dt <- as.matrix(wdbc2.dt$diagnosis)
xwdbc2.dt <- prepare.glmnet(wdbc2.dt[,!"diagnosis"])


#fit ridge regression model with cross validation
fit.ridge <- glmnet(xwdbc2.dt, ywdbc2.dt, family="binomial", alpha=0, subset = train.idx)
#fit regression with lasso penalty
fit.lasso <- glmnet(xwdbc2.dt, ywdbc2.dt, family="binomial", subset = train.idx)

#plotting coefficients for lambda regression
plot(fit.ridge, main="Ridge trajectories")
plot(fit.lasso, main="Lasso trajectories")

#learning by cross validation
fit.cv.ridge <- cv.glmnet(xwdbc2.dt, ywdbc2.dt, family="binomial", alpha=0, subset = train.idx)
fit.cv.lasso <- cv.glmnet(xwdbc2.dt, ywdbc2.dt, family="binomial", subset = train.idx)

#Plotting the trajectory of the coefficients
plot(fit.cv.ridge, main="Ridge")
plot(fit.cv.lasso, main="Lasso") 



```
## (b)

Optimal AUC is given when lambda is at its minimum, ergo the maximum of cvm is the AUC corresponding to the optimal lambda
```{r}
#lambda for ridge regression model
max(fit.cv.ridge$cvm)
#lambda for lasso regression model
max(fit.cv.lasso$cvm)


#AUC for best lambda for ridge regression
fit.cv.ridge$cvm[which(fit.cv.ridge$lambda == fit.cv.ridge$lambda.min)]
#AUC for best lambda for lasso regression
fit.cv.lasso$cvm[which(fit.cv.lasso$lambda == fit.cv.lasso$lambda.min)]



#AUC within 1 sd of the maximum
#lasso regression
fit.cv.lasso$cvm[which(fit.cv.lasso$lambda == fit.cv.lasso$lambda.1se)]


#ridge regression
fit.cv.ridge$cvm[which(fit.cv.ridge$lambda == fit.cv.ridge$lambda.1se)]

```
## (c)
```{r}

#data table for ridge regression
fit.ridge.dt <- data.table(model.lambda = signif(fit.cv.ridge$lambda,3),
                           model.size = fit.cv.ridge$nzero,
                           model.AUC = signif(fit.cv.ridge$cvm))

#data table for lasso regression
fit.lasso.dt <- data.table(model.lambda = signif(fit.cv.lasso$lambda,3),
                           model.size = fit.cv.lasso$nzero,
                           model.AUC = signif(fit.cv.lasso$cvm, 3))

#test on test data???


plot(fit.ridge.dt$model.lambda, fit.ridge.dt$model.AUC, 
     col = ifelse(fit.cv.ridge$cvm == max(fit.cv.ridge$cvm), "red", "black"))

plot(fit.lasso.dt$model.lambda, fit.lasso.dt$model.AUC,
     col = ifelse(fit.cv.lasso$cvm == max(fit.cv.lasso$cvm), "red", "black"))

```
## (d)

No specific model is specified so the assumption was made that a logistic regression model was meant.
```{r}
#import library
library(MASS)


#fit logistic regression model on same training data
fit.log <- glm(diagnosis ~ ., data = wdbc2.dt[, !"id"], family = "binomial", subset= train.idx )

#perform backward elimination
log.back <- stepAIC(fit.log, direction="back")

#required coefficients with their estimate
log.back$coefficients[-1] #excluding the intercept

length(log.back$coefficients[-1])

#HOW DO I STANDARDIZE THE COEFFICIENTS??????????

```

## (e)
```{r}

#difining the null model
null.forward <- glm(diagnosis ~ 1, data=wdbc2.dt[, !"id"], family="binomial", subset = train.idx)

#performing forward selection
log.forward <- stepAIC(null.forward, scope=list(upper=fit.log), direction="forward")

#no coefficients were selected and then later removed

#final coefficients
log.forward$coefficients[-1]

length(log.forward$coefficients[-1])
#how to standardize the coefficients

#summary(log.forward)

```
## (f) 
The best method for evaluation between the two models would be a log-likelihood test. However, since the two models are not nested, i.e. the forward selection model is not a special case of the backward elimination model, a log-likelihood analysis cannot  be performed. (see: https://www.statisticshowto.com/likelihood-ratio-tests/)
Therefore, I decided to compare the models based on Pseudo R squared or McFadden's R squared which gives the ratio of log-likelihoods between the current model and the null model
```{r}

#null model has already been defined during forward selection : null.forward

#McFadden's R squared for forward selection
1-logLik(log.forward)/logLik(null.forward)

#McFadden's R squared for backward elimination
1-logLik(log.back)/logLik(null.back)



```
The backward model has a slightly better McFadden's score than the forward selection model. However, these differences are only minimal which might justify the selection of the coefficients selected during the forward selection model for future studies.

## (g)
```{r}
#making the test set
test <- wdbc2.dt[-train.idx]

#forward selection predicted probabilities (right)
pred.prob.forward <- predict(log.forward, newdata=wdbc2.dt[train.idx], type="response")

#AUC for forward selection
roc(wdbc2.dt$diagnosis[train.idx], pred.prob.forward, data=wdbc2.dt[-train.idx])$auc

#predicted probabilities for backward elimination
pred.prob.backward <- predict(log.back, newdata = wdbc2.dt[train.idx], type="response")

#AUC for backward elimination
roc(wdbc2.dt$diagnosis[train.idx], pred.prob.backward, data=wdbc2.dt[-train.idx, ])$auc

```
## (h)
Ridge Regression AUC
```{r}

# ridge regression
pred.prob.test.ridge <- predict(fit.cv.ridge, newx = xwdbc2.dt[-train.idx,], type = "response", s=fit.cv.ridge$lambda.1se)

roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.ridge, data=wdbc2.dt[train.idx])$auc

```
Lasso Regression AUC
```{r}
#lasso regression
pred.prob.test.lasso <- predict(fit.cv.lasso, newx = xwdbc2.dt[-train.idx,], type = "response", s=fit.cv.lasso$lambda.1se)

roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.lasso, data=wdbc2.dt[train.idx])$auc

```
Backward Elimination AUC
```{r}


#backward elimination

#predicted probabilities for backward elimination
pred.prob.test.backward <- predict(log.back, newdata = wdbc2.dt[-train.idx], type="response")

#AUC for backward elimination
roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.backward, data=wdbc2.dt[train.idx])$auc

```
Forward Selection AUC
```{r}

#forward selection

#forward selection predicted probabilities (right)
pred.prob.test.forward <- predict(log.forward, newdata=wdbc2.dt[-train.idx], type="response")

#AUC for forward selection
roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.forward, data=wdbc2.dt[train.idx])$auc


```

Plotting the test AUC.
```{r}
#plotting the test AUC's
roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.forward, data=wdbc2.dt[train.idx],lagacy.axes=TRUE, plot = TRUE, lwd=1, main="Plot of Test AUC's", asp=NA)

roc(wdbc2.dt$diagnosis[-train.idx], pred.prob.test.backward, data=wdbc2.dt[train.idx], plot = TRUE, add=TRUE, col="orange", lwd=1)

roc(wdbc2.dt$diagnosis[-train.idx], as.vector(pred.prob.test.lasso), data=wdbc2.dt[train.idx], plot = TRUE, add=TRUE, col="red", lwd=1)

roc(wdbc2.dt$diagnosis[-train.idx], as.vector(pred.prob.test.ridge), data=wdbc2.dt[train.idx], plot=TRUE, add=TRUE, col="blue", lwd=1)

legend("bottomright", 
       c("Forward Elimination", "Backward Elimination", "Lasso Regression", "Ridge Regression"),
       fill = c("black", "orange", "red", "blue"))


```
Coparison between training and test AUC's:\n
\n
Ridge Regression:\n
Training: 0.4345539\n
Testing: 0.9895\n
The training AUC is significantly smaller than the the testing AUC which is expected since ridge regression introduces a small bias to not overfit the model on the training set. Therefore, this behaviour is normal and expected.

\n
Lasso Regression:\n
Training: 0.4504536\n
Testing: 0.9915\n
Similarly to the ridge regression, the lasso regression introduces a small bias to fit the test set better. Therefore, the training AUC is much greater than the test AUC. 
\n

Backward Elimination:\n
Training: 0.9717\n
Testing: 0.9855\n
Even though this model does not overfit the data (since Training AUC < Testing AUC), the general performance compared to the other three models is lower. It seems that the elminiation of coefficients slightly adds to the performance of the model. This is in contrast to the McFadden R square test performed in part (f) showing that the McFadden's score is not a good way of comparing the forward and backward model.

\n

Forward Elimination:\n
Training: 0.9891\n
Testing:0.9911\n
Similarly to the backward elimination model, the model is not overfit since the AUC increases from training to testing. 

